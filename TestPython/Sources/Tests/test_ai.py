import numpy as np
import time
from PIL import ImageGrab
import cv2
import gym
from gym import spaces
from pynput import mouse, keyboard
import torch
from stable_baselines3 import DQN # Deep Q-Networkã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
import pytk as tk

# --- 1. å®šç¾© ---
mouse_ctrl = mouse.Controller()
kb_ctrl = keyboard.Controller()

# å®šç¾©å¯èƒ½ãªé›¢æ•£çš„ãªè¡Œå‹• (ä¾‹: 10å€‹ã®é¸æŠè‚¢)
ACTION_MAP = {
    0: ('mouse_move_relative', 10, 0),    # å³ã«10pxç§»å‹•
    1: ('mouse_move_relative', -10, 0),   # å·¦ã«10pxç§»å‹•
    2: ('mouse_move_relative', 0, 10),    # ä¸‹ã«10pxç§»å‹•
    3: ('mouse_move_relative', 0, -10),   # ä¸Šã«10pxç§»å‹•
    4: ('mouse_click', 'left'),           # å·¦ã‚¯ãƒªãƒƒã‚¯
    5: ('key_press', 'enter'),            # Enterã‚­ãƒ¼ã‚’æŠ¼ã™
    6: ('key_press', 'tab'),              # Tabã‚­ãƒ¼ã‚’æŠ¼ã™
    # ... ä»–ã®é‡è¦ãªæ“ä½œ ...
    7: ('no_op', 0),                      # ä½•ã‚‚ã—ãªã„
}

# --- 2. ã‚«ã‚¹ã‚¿ãƒ  Gym ç’°å¢ƒã®ä½œæˆ ---

class ScreenRPAEnv(gym.Env):
    """PCç”»é¢æ“ä½œã®ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ å¼·åŒ–å­¦ç¿’ç’°å¢ƒ"""
    metadata = {'render.modes': ['human']}

    def __init__(self):
        super(ScreenRPAEnv, self).__init__()

        # çŠ¶æ…‹ç©ºé–“: ç”»é¢ã‚­ãƒ£ãƒ—ãƒãƒ£ (ä¾‹: 84x84 ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«)
        self.observation_space = spaces.Box(
            low=0, high=255, shape=(1, 84, 84), dtype=np.uint8
        )
        
        # è¡Œå‹•ç©ºé–“: å®šç¾©ã•ã‚ŒãŸè¡Œå‹•ã®æ•° (ACTION_MAPã®ã‚µã‚¤ã‚º)
        self.action_space = spaces.Discrete(len(ACTION_MAP))
        
        # æœ€çµ‚ç›®æ¨™ã®åº§æ¨™ (ä¾‹: ç”»é¢ä¸­å¤®ã®ãƒœã‚¿ãƒ³)
        self.target_area = (400, 400, 600, 600) 
        self.max_steps = 50 # 1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°
        self.current_step = 0

    def _get_observation(self):
        """ç¾åœ¨ã®ç”»é¢ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”¨ã«å‰å‡¦ç†ã™ã‚‹"""
        img = ImageGrab.grab()
        img_np = np.array(img)
        
        # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã«å¤‰æ›ã—ã€ãƒªã‚µã‚¤ã‚º (DRLã§ã‚ˆãä½¿ã‚ã‚Œã‚‹84x84)
        gray_img = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
        resized_img = cv2.resize(gray_img, (84, 84), interpolation=cv2.INTER_AREA)
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¥åŠ›å½¢å¼ (1, 84, 84) ã«æ•´å½¢ã—ã€æ­£è¦åŒ–ã¯ãƒ¢ãƒ‡ãƒ«ã«ä»»ã›ã‚‹
        return np.expand_dims(resized_img, axis=0) 

    def _compute_reward(self):
        """å ±é…¬ã‚’è¨ˆç®—ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ (å¼·åŒ–å­¦ç¿’ã®éµã¨ãªã‚‹éƒ¨åˆ†)"""
        
        # ãƒã‚¦ã‚¹ãŒç›®æ¨™ã‚¨ãƒªã‚¢ã«è¿‘ã¥ã„ãŸã‚‰å ±é…¬ã‚’ä¸ãˆã‚‹
        x, y = mouse_ctrl.position
        
        # ç›®çš„ã®ãƒœã‚¿ãƒ³ä¸Šã«ãƒã‚¦ã‚¹ãŒã‚ã‚‹å ´åˆ
        if self.target_area[0] < x < self.target_area[2] and \
           self.target_area[1] < y < self.target_area[3]:
            return 1.0 # æ­£ã®å ±é…¬
        else:
            return -0.01 # ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ã‚ãšã‹ãªè² ã®å ±é…¬ (æ—©ãç›®çš„ã‚’é”æˆã•ã›ã‚‹ãŸã‚)

    def step(self, action):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã‚’å—ã‘ä»˜ã‘ã€æ¬¡ã®çŠ¶æ…‹ã€å ±é…¬ã€çµ‚äº†ãƒ•ãƒ©ã‚°ã‚’è¿”ã™"""
        
        self.current_step += 1
        
        # 1. è¡Œå‹•ã®å®Ÿè¡Œ
        action_type, *params = ACTION_MAP[action]
        
        if action_type == 'mouse_move_relative':
            dx, dy = params
            x, y = mouse_ctrl.position
            mouse_ctrl.position = (x + dx, y + dy)
        elif action_type == 'mouse_click':
            # å®Ÿéš›ã«ã¯ã‚¯ãƒªãƒƒã‚¯å¾Œã«ç”»é¢ãŒå¤‰åŒ–ã™ã‚‹ã¯ãš
            mouse_ctrl.click(mouse.Button.left, 1)
        elif action_type == 'key_press':
            key_val = params[0]
            if key_val == 'enter':
                 kb_ctrl.press(keyboard.Key.enter)
                 kb_ctrl.release(keyboard.Key.enter)
            # ... ä»–ã®ã‚­ãƒ¼æ“ä½œ ...

        time.sleep(0.1) # ç”»é¢ã®å¤‰åŒ–ã‚’å¾…ã¤

        # 2. æ¬¡ã®çŠ¶æ…‹ã®å–å¾—
        observation = self._get_observation()
        
        # 3. å ±é…¬ã®è¨ˆç®—
        reward = self._compute_reward()
        
        # 4. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†åˆ¤å®š
        done = self.current_step >= self.max_steps 
        # ã¾ãŸã¯ã€ç›®çš„ã®æ“ä½œãŒå®Œäº†ã—ãŸã‹ã©ã†ã‹ã‚’ç”»é¢èªè­˜ã§åˆ¤æ–­
        
        info = {}
        return observation, reward, done, info

    def reset(self, **kwargs):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹æ™‚ã«ç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹"""
        self.current_step = 0
        # å®Ÿéš›ã®RPAã§ã¯ã€ãƒªã‚»ãƒƒãƒˆæ™‚ã«ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’åˆæœŸçŠ¶æ…‹ã«æˆ»ã™æ“ä½œãŒå¿…è¦
        print("--- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ ãƒªã‚»ãƒƒãƒˆ ---") 
        return self._get_observation()

    def render(self, mode='human'):
        """äººé–“ãŒè¦³å¯Ÿã§ãã‚‹ã‚ˆã†ã«ç’°å¢ƒã‚’æç”» (ä»Šå›ã¯ä¸è¦ã ãŒã€Gymã®æ¨™æº–ãƒ¡ã‚½ãƒƒãƒ‰)"""
        pass

    def close(self):
        """ç’°å¢ƒã‚’é–‰ã˜ã‚‹"""
        pass


# --- 3. å­¦ç¿’ã®å®Ÿè¡Œ ---

def run_learning_with_drl():
    """å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹"""
    
    print("ğŸ§  å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–ã¨å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # 1. ç’°å¢ƒã®ä½œæˆ
    env = ScreenRPAEnv()
    
    # 2. DRLãƒ¢ãƒ‡ãƒ«ã®å®šç¾© (PyTorchãƒ™ãƒ¼ã‚¹ã®DQN)
    # policy='CnnPolicy'ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€ç”»åƒå…¥åŠ›ã«é©ã—ãŸç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯(CNN)ã‚’ä½¿ç”¨
    model = DQN("CnnPolicy", env, verbose=1, 
                learning_rate=1e-4, 
                buffer_size=10000, 
                learning_starts=1000,
                device="cuda" if torch.cuda.is_available() else "cpu")

    # 3. å­¦ç¿’ã®å®Ÿè¡Œ
    # ã“ã“ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç”»é¢ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚’çŠ¶æ…‹ã¨ã—ã¦å—ã‘å–ã‚Šã€æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’ã—ã¾ã™ã€‚
    # å®Ÿéš›ã®RPAã§ã¯ã€æ•°ç™¾ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ã®å­¦ç¿’ãŒå¿…è¦ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
    try:
        model.learn(total_timesteps=10000) 
    except Exception as e:
        print(f"å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
    # 4. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    model.save("rpa_drl_model")
    print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ rpa_drl_model.zip ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")


# --- 4. å®Ÿè¡Œ (æ¨è«–) ã®å®Ÿè¡Œ ---

def run_execution_with_drl():
    """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€æ“ä½œã‚’å®Ÿè¡Œã™ã‚‹"""
    print("â–¶ï¸ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ“ä½œå®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã™...")
    
    try:
        env = ScreenRPAEnv()
        # 1. ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        model = DQN.load("rpa_drl_model", env=env)
        
        obs, _ = env.reset()
        done = False
        
        while not done:
            # 2. ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹è¡Œå‹•é¸æŠ (æ¨è«–)
            action, _states = model.predict(obs, deterministic=True)
            
            # 3. è¡Œå‹•ã®å®Ÿè¡Œã¨æ¬¡ã®çŠ¶æ…‹ã¸
            obs, reward, done, info = env.step(action)
            
            print(f"è¡Œå‹•: {ACTION_MAP[action]}, å ±é…¬: {reward}, çµ‚äº†: {done}")
            
    except FileNotFoundError:
        print("ã‚¨ãƒ©ãƒ¼: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« 'rpa_drl_model.zip' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    except Exception as e:
        print(f"å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
    env.close()

# --- 5. GUI (ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹) ---

def create_gui():
    """GUIã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä½œæˆã™ã‚‹"""
    root = tk.Tk()
    root.title("DRL RPA ãƒ„ãƒ¼ãƒ«")
    root.geometry("350x180")
    
    lbl = tk.Label(root, text="å¼·åŒ–å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„")
    lbl.pack(pady=10)
    
    # å­¦ç¿’ãƒœã‚¿ãƒ³
    btn_learn = tk.Button(root, text="ğŸ§  å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ (DRLã§æ“ä½œã‚’å­¦ç¿’)", command=run_learning_with_drl, height=2)
    btn_learn.pack(pady=5, padx=20, fill='x')
    
    # å®Ÿè¡Œãƒœã‚¿ãƒ³
    btn_execute = tk.Button(root, text="â–¶ï¸ å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ (å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§æ“ä½œ)", command=run_execution_with_drl, height=2)
    btn_execute.pack(pady=5, padx=20, fill='x')
    
    root.mainloop()

if __name__ == "__main__":
    create_gui()